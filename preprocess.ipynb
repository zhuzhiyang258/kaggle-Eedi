{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_file = \"dahta/train .csv\"\n",
    "mapping_file = \"data/misconception_mapping.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_file)\n",
    "mapping_df = pd.read_csv(mapping_file)\n",
    "print(\"Train Data Preview:\")\n",
    "print(train_df.head())\n",
    "print(\"\\nMapping Data Preview:\")\n",
    "print(mapping_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map the correct answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to map CorrectAnswer to corresponding AnswerText\n",
    "def map_correct_answer(row):\n",
    "    # Map the correct answer to the corresponding column value\n",
    "    answer_column = f\"Answer{row['CorrectAnswer']}Text\"\n",
    "    return row[answer_column]\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "train_df = pd.read_csv(train_file)\n",
    "train_df['CorrectAnswerText'] = train_df.apply(map_correct_answer, axis=1)\n",
    "\n",
    "# Display the result\n",
    "train_df[['CorrectAnswer', 'CorrectAnswerText']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_misconception(misconception_id, mapping):\n",
    "    if pd.notna(misconception_id):\n",
    "        return mapping.get(misconception_id, \"Unknown\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map the misconception names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to map misconception IDs to names\n",
    "misconception_dict = mapping_df.set_index('MisconceptionId')['MisconceptionName'].to_dict()\n",
    "train_df['MisconceptionA'] = train_df['MisconceptionAId'].apply(map_misconception, args=(misconception_dict,))\n",
    "train_df['MisconceptionB'] = train_df['MisconceptionBId'].apply(map_misconception, args=(misconception_dict,))\n",
    "train_df['MisconceptionC'] = train_df['MisconceptionCId'].apply(map_misconception, args=(misconception_dict,))\n",
    "train_df['MisconceptionD'] = train_df['MisconceptionDId'].apply(map_misconception, args=(misconception_dict,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nMapped Misconception Names:\")\n",
    "print(train_df[['MisconceptionA', 'MisconceptionB', 'MisconceptionC', 'MisconceptionD']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataframe into separate rows for each answer\n",
    "split_rows = []\n",
    "for _, row in train_df.iterrows():\n",
    "    split_rows.append({\n",
    "        'QuestionId_Answer': f\"{row['QuestionId']}_A\",\n",
    "        'QuestionId': row['QuestionId'],\n",
    "        'ConstructId': row['ConstructId'],\n",
    "        'ConstructName': row['ConstructName'],\n",
    "        'CorrectAnswer': row['CorrectAnswer'],\n",
    "        'SubjectId': row['SubjectId'],\n",
    "        'SubjectName': row['SubjectName'],\n",
    "        'QuestionText': row['QuestionText'],\n",
    "        'AnswerText': row['AnswerAText'],\n",
    "        'MisconceptionId': row['MisconceptionAId'],\n",
    "        'MisconceptionName': row['MisconceptionA'],\n",
    "        'CorrectAnswerText': row['CorrectAnswerText']\n",
    "\n",
    "    })\n",
    "    split_rows.append({\n",
    "        'QuestionId_Answer': f\"{row['QuestionId']}_B\",\n",
    "        'QuestionId': row['QuestionId'],\n",
    "        'ConstructId': row['ConstructId'],\n",
    "        'ConstructName': row['ConstructName'],\n",
    "        'CorrectAnswer': row['CorrectAnswer'],\n",
    "        'SubjectId': row['SubjectId'],\n",
    "        'SubjectName': row['SubjectName'],\n",
    "        'QuestionText': row['QuestionText'],\n",
    "        'AnswerText': row['AnswerBText'],\n",
    "        'MisconceptionId': row['MisconceptionBId'],\n",
    "        'MisconceptionName': row['MisconceptionB'],\n",
    "        'CorrectAnswerText': row['CorrectAnswerText']\n",
    "    })\n",
    "    split_rows.append({\n",
    "        'QuestionId_Answer': f\"{row['QuestionId']}_C\",\n",
    "        'QuestionId': row['QuestionId'],\n",
    "        'ConstructId': row['ConstructId'],\n",
    "        'ConstructName': row['ConstructName'],\n",
    "        'CorrectAnswer': row['CorrectAnswer'],\n",
    "        'SubjectId': row['SubjectId'],\n",
    "        'SubjectName': row['SubjectName'],\n",
    "        'QuestionText': row['QuestionText'],\n",
    "        'AnswerText': row['AnswerCText'],\n",
    "        'MisconceptionId': row['MisconceptionCId'],\n",
    "        'MisconceptionName': row['MisconceptionC'],\n",
    "        'CorrectAnswerText': row['CorrectAnswerText']\n",
    "    })\n",
    "    split_rows.append({\n",
    "        'QuestionId_Answer': f\"{row['QuestionId']}_D\",\n",
    "        'QuestionId': row['QuestionId'],\n",
    "        'ConstructId': row['ConstructId'],\n",
    "        'ConstructName': row['ConstructName'],\n",
    "        'CorrectAnswer': row['CorrectAnswer'],\n",
    "        'SubjectId': row['SubjectId'],\n",
    "        'SubjectName': row['SubjectName'],\n",
    "        'QuestionText': row['QuestionText'],\n",
    "        'AnswerText': row['AnswerDText'],\n",
    "        'MisconceptionId': row['MisconceptionDId'],\n",
    "        'MisconceptionName': row['MisconceptionD'],\n",
    "        'CorrectAnswerText': row['CorrectAnswerText']\n",
    "    })\n",
    "\n",
    "split_df = pd.DataFrame(split_rows)\n",
    "split_df = split_df.dropna(subset=['MisconceptionName'])\n",
    "print(\"\\nSplit Data Preview:\")\n",
    "print(split_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, item in split_df.iterrows():\n",
    "    print(f\"QuestionText: {item['QuestionText']}, ConstructName: {item['ConstructName']},CorrectAnswerText: {item['CorrectAnswerText']}, AnswerText: {item['AnswerText']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt import math_problems_anlysis_prompt\n",
    "\n",
    "# Generate the prompt for each row in the DataFrame\n",
    "def generate_prompt(row):\n",
    "    return math_problems_anlysis_prompt.format(\n",
    "        question=row['QuestionText'], \n",
    "        construct=row['ConstructName'], \n",
    "        goodanswer=row['CorrectAnswerText'], \n",
    "        badanswer=row['AnswerText']\n",
    "    )\n",
    "\n",
    "prompts = split_df.apply(generate_prompt, axis=1)\n",
    "\n",
    "# Print the prompts\n",
    "print(\"\\nPrompts:\")\n",
    "prompts.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = prompts.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from service.llms_service import LLM_vllm_Service \n",
    "# model_path = 'models/raw_model/mistral'\n",
    "# tensor_parallel_size = 2\n",
    "# llm_service = LLM_vllm_Service(model_path, tensor_parallel_size)\n",
    "# question = [prompts[1]]\n",
    "# answer = llm_service.generate(question, llm_service.sampling_params)\n",
    "# print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
